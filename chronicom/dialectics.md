---
layout: post
title: Dialectics
description: The Chronicles of Computation, Vol. I
---

[The Chronicles of Computation]({{ site.baseurl }}/#chronicom), Vol. I

## Important Notice

**This document is currently a work in progress. Please refrain from sharing it before it gets published, for ensuring an optimal reading experience.** Still, if you are reading this in the interval of 5-19 January, consider joining me in the interdisciplinary audit documented in the final chapter by applying [here](https://aisafety.camp/).

## Reception

Experts have been asked to bet social capital on this volume's relevance and usefulness through a brief public review. Should respondents (1) endorse this work when it actually turns out to be irrelevant, or (2) fail to endorse it when it actually turns out to be useful, then they would find themselves in an unfavorable position. Conversely, it would be favorable for them to make qualitative assessments which match reality. This first section can be seen as a spin on the peer review process inspired by prediction markets, meant to communicate to the reader an assessment of this work's standing informed by experts who are incentivized to weigh in sensibly.

## Summary of Contents

In the first half of this volume, we slowly build towards a theoretical framework of language model reasoning centered around dialectics,{% include sidenote.html id="mn-dialects" note='The term can refer to a multitude of things. Nicholas Rescher opens his _Dialectics_ by arguing that "it is, as it were, the alchemy of philosophy. It is all things to all men: to some, the most rigorous procedure for exact and cogent thinking; to others, a way of getting outside the established rules—an "anything goes" process for breaking through to unfettered innovations of thinking. For some it is the quintessential method of inquiring thought, for others the quintessential antimethod." We limit ourselves here to the meaning of a regimented dialogue between parties which is genarally held explicitly.'%} the age-old practice of truth-seeking through regimented dialogue. In the second half, we attempt to apply the framework by exploring a number of specific use cases in AI safety. Side notes are used extensively to elaborate on jargon in the volume's body, in order to ensure that a broader readership is, both literally and figuratively, on the same page.

### Ch. I, Dialectical Power Dynamics

In the first chapter, we devise an automated way of evaluating parties engaged in a debate of arbitrary length held in natural language. Our algorithm is inspired by the argumentation-theoretic notion of pragmatic validity and the epistemological notion of coherentism, yet its concrete implementation relies on heuristics for node centrality from network theory.

1. [Kaleidoscope of Reasonableness](#kaleidoscope-of-reasonableness)
2. [Beliefs as Means or Ends](#beliefs-as-means-or-ends)
3. [Carving the Algorithm](#carving-the-algorithm)
4. [ArgRank](#argrank)

### Ch. II, Deliberative Arms Race

In the second chapter, we describe in detail the process of obtaining DebateGPT, a language model fine-tuned to simulate increasingly pertinent debates by attempting to excel at the previously described evaluation. This novel training regime incorporates a self-play paradigm, runs mostly on synthetic data, and we assess has a non-zero chance of bootstrapping language model reasoning into superhuman territory, assuming a number of probable future advancements.

1. [Brief Review of Language Models](#brief-review-of-language-models)
2. [Training DebateGPT]()
3. [Hanson's Elephant]()
4. [Ephemeral Shards & Autocurricular Activities]()
5. [Climbing Schild's Ladder]()

### Ch. III, Bounded Defeasibility 

In the third chapter, we continue by framing the reasoning capabilities of language models such as DebateGPT in terms of _bounded defeasibility_, the amount of computational "firepower" a grouping of arguments can withstand before being invalidated. The amount of time at their disposal, the number of available tries, and the reasoning capabilities of the (simulated) agents in question represent some of those bounds.

1. [Brief Review of Non-Monotonic Logic]()
2. [Argument Is War]()
3. [Core Formalism]()

### Ch. IV, Deployment Strategies 

In the fourth chapter, we then go on to suggest a number of applications of this theoretical framework in AI safety which attempt to unify several recent avenues of investigation: debate, simulators, interpretability, interaction games, long reflection, shard theory, and others. Those deployment strategies are meant to scale in synchrony with the bootstrapped reasoning capabilities hinted at in previous chapters.

1. [Building on Cyborgism]()
2. [Building on Simulators & Metaethics]()
3. [Building on Interaction Games]()
4. [Building on Long Reflection]()
5. [Connections to Logical Inductors]()

### Ch. V, Interdisciplinary Audit

In the fifth and final chapter, we conduct a comprehensive interdisciplinary investigation of DebateGPT, in an attempt to gauge the feasibility of the applications described previously. We approach our object of study from angles as diverse as game theory and dynamical systems.

1. [Argumentation Theory]()
2. [Non-Monotonic Logic]()
3. [Sociology]()
4. [Game Theory]()
5. [Dynamical Systems]()

## Acknowledgements

Paul Bricman would like to thank the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) for financial support during the project, [Stability AI](https://stability.ai/) for providing the computational resources necessary to train DebateGPT, [Conjecture](https://www.conjecture.dev/) for providing the space to explore related ideas during a previous research fellowship, and [AI Safety Camp](https://aisafety.camp/) for providing context and resources for the entire team to investigate DebateGPT.

## Ch. I, Dialectical Power Dynamics

### Kaleidoscope of Reasonableness

Ask a scholar of pure mathematics, computer science, or formal logic what makes an instance of argumentation valid, and they will likely highlight the relevance of making sure that the conclusion follows logically from the premises for each individual reasopning step. More often than not, this implies relying on a host of approved types of inference (e.g. [modus ponens](https://en.wikipedia.org/wiki/Modus_ponens)), while making sure to steer clear of degenerate ones (i.e. fallacies). This conception of reasonableness is often referred to as "geometrical," due to its appeal to only build on solid premises and construct arguments using an idealized set of operations.

Ask a scholar of argumentation theory the same question, and—following a cursory smile indicating they have long been waiting for this—they will likely not hesitate to point out that there have been multiple prominent schools of thought over time which advocated different, often incompatible, conceptions of reasonableness. Each one is backed by a different rationale, has its own features and shortcomings, and has emerged in a different cultural setting, often separated by thousands of years and kilometers. Let us briefly sample this range of conceptions.

For instance, Perelman and Olbrechts-Tyteca suggested a conception of reasonableness grounded in rhetoric. According to it, an instance of argumentation is valid if and only if it succeeds in persuading a group of individuals of its conclusion. While the most visible shortcoming of this conception is that it can quickly degenerate into sophism,{% include sidenote.html id="mn-sophism" note='The [sophists](https://en.wikipedia.org/wiki/Sophist) were teachers of rhetoric in ancient Greece, notorious for "equipping" individuals with techniques for making a strong case in court, regardless of the appropriateness of their position. Being sneered at by virtually all their contemporary philosophers, they were frowned upon for not seeking wisdom, but merely monetizing the skill of persuasion as a means of taking advantage of others.'%} its proponents highlight the possibility of grounding reasonableness in the persuasion of a particularly _rational_ audience. This litmus test can be further extended to involve the persuasion of an idealized omniscient agent, but also of oneself, by framing self-deliberation as self-persuasion.

<div class='epigraph'><blockquote><p>Indeed, the object of the theory of argumentation is the study of the discursive techniques allowing us to induce or to increase the mind's adherence to the theses presented for its assent.</p>
<div>Chaïm Perelman & Lucie Olbrechts-Tyteca,<cite> The New Rhetoric</cite></div></blockquote></div>

As a different example, Toulmin and the school of thought which emerged around his ideas advocated for a conception of reasonableness which incorporates domain-specificity. If the geometrical conception often requires abstracting statements into propositional atoms (i.e. $$P$$ could equally well denote _"All men are mortal."_ and _"Socrates is a man."_), Toulmin argues that arguments are often _substantial_, relying on domain-specific means of warranting conclusions, as opposed to standardized  _analytical_ operations on abstracted symbols. For instance, the fact that a study in the natural sciences has conformed to best practices in terms of replicability and reprodicibility can be used to back its findings. In contrast, people working in pure mathematics might not rely on peer-reviewed empirical studies to back theorems, but might want to verify proofs using software. The practices of ensuring sound reasoning in finance are yet again different, relying more on computer simulations and historical performance. Several spin-off schools of thought echoed Toulmin's dissatisfaction regarding the limited practical reach of the highly-analytical formal logic, including the slightly chaotic field of _informal logic_.

<div class='epigraph'><blockquote><p>A man demonstrates his rationality, not by a commitment to fixed ideas, stereotyped procedures, or immutable concepts, but by the manner in which, and the occasions on which, he changes those ideas, procedures, and concepts.</p>
<div>Stephen Toulmin,<cite> Human Understanding</cite></div></blockquote></div>

To expand our collection of reasonableness conceptions even further, the pragma-dialectical framework developed by van Eemeren and Grootendorst grounds reasonableness in dialectics. In this context, an instance of argumentation is valid if and only if there exists no strategy to be employed by an opponent in a regimented dialogue which manages to undermine it. The proponents of this framework are particularly interested in enabling effective reasoning in a wide range of situations, rather than only in some higher realm of abstractions. That is why the conception of reasonableness on which their framework rests has a major pragmatic component. The regimented dialogue can be carried out by real individuals and can target a wide range of matters, from the most mundane to the most consequential. The ruleset of available tactics is simply made available to the individuals engaged in dialogue at the beginning, while strategies can be as diverse as forcing the opponent into self-contradiction or exploiting their (involuntary) support. That said, the framework can be brought closer to formal dialectics in order to account for idealized reasoning by employing perfectly rational agents as discussants, in a move similar to that of Perelman and Olbrechts-Tyteca.

<div class='epigraph'><blockquote><p>Accordingly, the prime aims of the present discussion are to exhibit the sociocommunal roots of the foundations of rationality, to provide an instrument for the critique of scepticism implicit in the cognitive solipsism of the Cartesian approach, and to illuminate the communal and controversy-oriented aspects of argumentation and inquiry—scientiﬁc inquiry in particular.</p>
<div>Nicholas Rescher,<cite> Dialectics</cite></div></blockquote></div>

Indeed, for many centuries at a time, logic has been but part of dialectics, rather than a field of its own.{% include sidenote.html id="mn-ma" note='For instance, during the Middle Ages. Refer to _Section 2.10.1_ of the [Handbook of Argumentation Theory](https://link.springer.com/referencework/10.1007/978-90-481-9473-5) for a more detailed account.'%} The hot and cold relationship of the two disciplines over the centuries has perhaps been the closest thing argumentation theory has ever had to juicy gossip, with rhetoric a controversial element to complete the triad. It is difficult to overstate the reliance of contemporary mathematics, both pure and applied, on the foundation of formal logic, and so the very idea of scholars erecting an edifice of theory on a different foundation tends to induce vertigo. The very possibility of notions as elementary as conjunction, disjunction, and negation to be defined on the basis of a regimented dialogue instead of a logic{% include sidenote.html id="mn-logics" note='The term _logic_ is used here as countable in reference to the broad range of [three-valued](https://en.wikipedia.org/wiki/Three-valued_logic), [four-valued](https://en.wikipedia.org/wiki/Four-valued_logic), [many-valued](https://en.wikipedia.org/wiki/Many-valued_logic), and [modal](https://en.wikipedia.org/wiki/Modal_logic) logics which compete with [classical two-valued logic](https://en.wikipedia.org/wiki/Classical_logic).'%} sounds exceedingly exotic to the contemporary ear, outside a handful of niches like ludics, game semantics, and interactive computation.

To bring this section to an end, we have completed a very brief tour of several conceptions of reasonableness with the purpose of highlighting the breadth of approaches devised by scholars through the ages. Determining which conception of reasonableness is itself more reasonable is the subject of vigorous debate in present-day argumentation theory. Each conception appears better suited to deal with certain aspects of argumentation, while lacking in other respects. In the rest of this chapter, we will build on top of many of the conceptions listed above, in an attempt to develop an automated pipeline for estimating reasonableness as a single floating-point number.

### Beliefs as Means or Ends

All of the three disciplines which make up the umbrella field of argumentation theory (i.e. logic, dialectic, and rhetoric) can be said to house both work which frames reasoning as a means of reaching a conclusion based on beliefs (i.e. outcome-based), but also work which frames reasoning as a continuous process of updating and forming beliefs (i.e. process-based). The first can be seen as a building block of the latter, yet the latter can also be seen as a prerequisite of the former. However, those connections become somewhat counterproductive when considering the fact that the very same procedures (i.e. sketching out a proof, engaging in a dialogue) in their entirety can be motivated by both perspectives in different contexts.

In logic, writing a proof involves mainly chasing after a conclusion. How one actually navigates the "game tree" of available moves in search of the finish line is up to the logician in question. However, it is only of secondary relevance that each step of the proof involves obtaining an entirely new well-formed formula.{% include sidenote.html id="mn-wff" note='This [technical term](https://en.wikipedia.org/wiki/Well-formed_formula) describes a collection of propositional atoms (e.g. $$P$$) brought together by connectives (e.g. $$¬$$, also known as negation) in a "legal" way (e.g. $$¬P$$, read "not P", instead of $$P¬$$). It is a syntactic constraint on how such elements are arranged, preventing the equivalent of a logician being able to experess _"flies bird."_ It is more relevant for being able to construct interesting proofs, rather than making sure logicians write correctly.'%} They are merely intermediate steps required to succeed in proving a certain conclusion. In other words, means.

<figure class="maincolumn">
    <figcaption class="marginnote"><a href="https://en.wikipedia.org/wiki/Fitch_notation">Fitch-style logic proof</a> with three premises. Each step of the proof involves obtaining an intermediate well-formed formula using an approved operation. For instance, double negation is used to "cancel out" the two chained negations from step 4, thus arriving at step 5.</figcaption>
    <img class="proof" src="{{ site.baseurl }}/assets/img/logicproof.svg" width="70%"/>
</figure>

In contrast, several direct applications of this formalism involve a much greater focus on the "beliefs" formed as the reasoning process unfolds. Expert systems had been a major topic in early symbolic AI, and they involved constantly expanding a [knowledge base](https://en.wikipedia.org/wiki/Knowledge_base) from a set of initial statements, using rules intimately tied to the ones above. An entire [inference engine](https://en.wikipedia.org/wiki/Inference_engine) was dedicated to the task of deriving a tiny bit of new knowledge from the knowledge which has been accumulated up to that point. Systems relying on [forward chaining](https://en.wikipedia.org/wiki/Forward_chaining) in particular involved trying to grow the knowledge base as much as the inference engine allowed before any other subsequent operation. The procedure is essentially identical to the previous one—there is only a shift in focus baked into the very ontology being used.{% include sidenote.html id="mn-ontology" note='_Ontology_ refers here to the conceptual framework which a specific intellectual tradition uses to deconstruct their object of study. Not to be confused with the kind of (digital) [knowledge bases](https://en.wikipedia.org/wiki/Ontology_(computer_science)) which tend to be hierarchical, also popular in early symbolic AI.'%}

<div class='epigraph'><blockquote><p>Reasoning is a transition in thought, where some beliefs provide the ground or reason for coming to another.</p>
<div>Jonathan Adler and Lance Rips,<cite> Reasoning</cite></div></blockquote></div>

The dichotomy of beliefs as means or ends is also echoed in the field of rhetoric. The closely-related enterprise of persuasion research investigates both ways of successfully persuading individuals of certain specific statements, but also the multi-stage process of getting there. The interactions between the advocated belief and the individual's previous epistemic baggage can become quite complex, making the act of persuasion better described as a multi-step intervention on an individual's belief system, prompting them to gradually "move" towards the advocated position by forming or discarding intermediate beliefs.

Perhaps in none of the three disciplines concerned with reasoning is the dichotomy more clear than in dialectics. On one hand, there are dialectical formalisms which focus entirely on a single statement supported by one proponent and contested by one opponent. The entire reason for being of a regimented dialogue cast in this light is to lead the discussants towards a conclusion regarding whether or not the statement in question is true. Similar to the logic proofs above, there are intermediate utterances made by the two parties as the dialogue unfolds—artifacts which are crucial for making the dialogue work in the first place—yet which are seen as mere scaffolding around the deliberation of the main statement.

However, things get much more colorful when looking at dialectical formalisms designed to be open-ended and perpetual. The "games" proposed by Jaakko Hintikka are perhaps the most salient example. For instance, Hintikka describes dialogues whose participants are motivated by an interest in being surprised and learning more, broadly referred to as "information-seeking dialogues."

<div class='epigraph'><blockquote><p>An answer to our problem can be given by making the payoff of the game for a given player dependent on the information-content of his (her) ﬁnal thesis (more properly speaking, the conjunction of all his theses). The more informative this thesis, the higher the payoff.</p>
<div>Jaakko Hintikka & Esa Saarinen,<cite> Information-seeking dialogues</cite></div></blockquote></div>


One's partner in such information-seeking dialogues must not necessarily be human, as illustrated below.

<div class='epigraph'><blockquote><p>We may think off "a" as a scientist or inquirer of some other kind and "b" as Nature or as a comparable impersonal source of information. [...] We may further think of "B" as a constant basic theory of "b" while the different choices of the "A" represent different hypotheses "a" is trying to prove by "putting questions to Nature."</p>
<div>Jaakko Hintikka,<cite> On the logic of an interrogative model of scientific inquiry</cite></div></blockquote></div>

Echoing Hintikka's almost literary move from _Man versus Man_ to _Man versus Nature_ in his dialectical dealings, while at the same time departing somewhat from Hintikka's reliance on information theory, the co-founders of the Erlangen School write as follows.{% include sidenote.html id="mn-logics" note='The "contructivist" school of thought, rather than the eponymous theological one, although as one can see, the line gets blurry at times. Also, one of them was [the founder of modern-day game semantics](https://en.wikipedia.org/wiki/Paul_Lorenzen) as well.'%}

<div class='epigraph'><blockquote><p>If one compares this agonistic origin of logic with modern conceptions, according to which logic is the system of rules that, whenever they are applied to some arbitrary true sentences, will lead one to further truths, then it will be but too obvious that the Greek agon has come to be a dull game of solitaire. In the original <b>two-person game</b> only God, secularized: “Nature,” who is in possession of all true sentences, would still qualify as an opponent. Facing Him there is the human individual – or perhaps the individual as a representative of humanity – devoted to the game of patience: Starting from sentences that were, so he believes, obtained from God before, or snatched away from Him, and following rules of logic, he is to gain more and more sentences.</p>
<div>Paul Lorenzen & Kuno Lorenz,<cite> Dialogische logik</cite></div></blockquote></div>

Nicholas Rescher takes this style of thinking even further by moving from one scientist engaged in truth-seeking to the whole scientific enterprise as a generalized "sociocommunal" process of deliberation about the nature of the world.

<div class='epigraph'><blockquote><p>At this stage, however, the social or communal aspect of the scientific enterprise comes crucially into play. For once a scientifically significant thesis is propounded by someone, the "scientific community" provides (1) certain opponents, in the form of self-appointed critics who challenge this thesis in an adversary manner, probing for its weak points and seeking to impede its acceptance, and (2) a larger, neutral body of concerned but otherwise uncommitted bystanders, who effectively act as arbiters of the "dispute."</p>
<div>Nicholas Rescher,<cite> Dialectics</cite></div></blockquote></div>

The idea of _a competition of ideas unfolding in the arena of society_ allows us to complete our impromptu sequence of conceptual hops while reaching an altogether different proto-discipline. The controversial field of memetics casts the beliefs which populate the collective consciousness in a Darwinian light. Belief systems are said to ruthlessly compete with one another for the scarce resource of human psyche. Instead of developing an immune system to fight off parasites, a belief system might "adapt" to prescribe the prohibition of "hosts" to adopt other beliefs. Particularly ambitious proponents of this perspective claim that culture in its totality can be explained in evolutionary terms, just how life has been explained to a surprising extent by evolutionary biology.{% include sidenote.html id="mn-memetics" note='Much of what is controversial about memetics is due to such observations being made not-so-tactfully in the context of religions as belief systems. For instance, one could say that preventing interfaith marriage or prescribing the same religion for children are adaptations of an ideology meant to protect or conquer psychological territory. One might imagine a more permissive belief system not standing the test of time.<br/><br>In contrast, the framing of memetics is significantly more influential in more secular circles (although there is vigorous criticism there, too). One might go so as far as to say that those communities are more "vulnerable" to being "infected" with the meme of memetics.'%}

While memetics and dialectics are worlds apart in terms of the employed formalisms and motivations, with dialectics relying on a carefully regimented procedure for effective reasoning while memetics relying on a supremely lax definition of spontaneous adaptations for understanding culture, the bridge between the two will prove key in later chapters. It will allow us to combine the rigidity of reasoning through regimented procedures with the evolutionary fluidity of models forged out of the selective pressures of empirical risk minimization.{% include sidenote.html id="mn-risk" note='[Technical term](https://en.wikipedia.org/wiki/Empirical_risk_minimization) employed in statistical learning theory to denote "training a model to perform well on the training data," but without all the ontological baggage associated with the anthropocentric metaphor of the model learning how to perform well on tasks as a person might.'%} This will become abundantly clear in the second half of _Chapter 2_, when we employ the connection as a conceptual building block, but will also resurface towards the end of _Chapter 4_, as we attempt to make the parallel more explicit.

To bring this section to an end, we have explored the pervasive dichotomy between beliefs as means and beliefs as ends, which appears to cut through virtually all disciplines concerned with the study of reasoning, and beyond. Going forward, we will include the flexibility necessary to accommodate both of those perspectives as a constraint for our algorithm.

### Carving the Algorithm

Previously, we have explored various ways in which scholars have conceived of the reasonableness of arguments. This will now serve us well, expanding the space of candidate algorithms which are backed by such a rationale—our raw material. We will make our way through this expansion by means of constraints, using them to cut down the search space. As we establish what our algorithm is _not_, the algorithm will slowly become crisper and better defined, each cut collapsing possibilities along some axis.

First, we would like the automated pipeline to be able to accomodate the richness of natural language. We would like to avoid the invariably lossy compression{% include sidenote.html id="mn-lossy" note='In contrast to _lossless_ compression, which can be reverted so as to perfectly reconstruct the original artifact, _lossy_ compression involves some about of information loss, meaning that perfect reconstruction becomes impossible, although getting e.g. 90% of the way is sufficient in many applications. For instance, [JPEG](https://en.wikipedia.org/wiki/JPEG) involves lossy image compression (with a configurable amount of loss, even), while [PNG](https://en.wikipedia.org/wiki/Portable_Network_Graphics) involves lossless image compression.'%} involved in converting beliefs into a brittle mosaic of propositional atoms, predicates, and connectives. Any such analytical statement can trivially be expressed in natural language (i.e. by describing it, albeit verbosely), while the reverse task has prompted an army of logics, each tailored to one very specific facet of reality (e.g. [temporal logic](https://en.wikipedia.org/wiki/Temporal_logic) for time), while still remaining "an open challenge." For sure, natural language itself is neither the perfect mirror, nor the very blueprint of the world, as many classics sincerely seemed to have hoped. Still, it is one less step of information loss from reality, and it is reality we are ultimately interested in reasoning about. Furthermore, while analytical statements might succeed in capturing essential features in highly structured domains, the notions we are most interested in when wielding unprecedented amounts of computation (e.g. human values, long-term flourishing) seem to resist being abstracted into a handful of sufficient statistics.{% include sidenote.html id="mn-sufficient" note='[Sufficient statistics](https://en.wikipedia.org/wiki/Sufficient_statistic) refer to the minimum number of measures which are enough to explain most of a statistical object. For instance, a "bell curve" distribution can be described in its entirety using two values: one measure of centrality and one measure of spread. For a more enthusiastic investigation of whether notions as messy and abstract as e.g. human values can be explained in full using a handful of appropriate factors, refer to [John Wentworth\'s agenda](https://www.alignmentforum.org/posts/Fut8dtFsBYRz8atFF/the-natural-abstraction-hypothesis-implications-and-evidence) for devising an appropriate objective for a powerful AI to chase safely.'%} {% include sidenote.html id="mn-demis" note='DeepMind leader Demis Hassabis, [quotes](https://www.youtube.com/watch?v=I5FrFq3W25U&t=2550s) the related neuro-symbolic integration challenge as the one they "spend most of [their] time thinking about," and notes that "we\'re still quite far from [solving it], and no one quite knows how to bridge that [neuro-symbolic] chasm. [...] It\'s a bit of a mystery."'%} Natural language, for better or worse, has evolved to serve us in communicating effectively about such topics, mediating much, if not most, of our culture.

<div class='epigraph'><blockquote><p>The medium is the message.</p>
<div>Marshall McLuhan,<cite> Understanding Media</cite></div></blockquote></div>

Let us take stock of the search space following the application of this first constraint. Unfortunately, we are largely forced to abandon the geometrical conception as a motivating rationale to base our algorithm on, not because of its elegance or crispness, which we are disheartened to leave behind, but because of its limited set of legal inferences being better suited for highly-structured domains rather than the messy world as a whole. The critical conceptions make up some of the remaining options, defining reasonable instances of argumentation as those which systematically resist being undermined by opponents. However, having largely left behind the foundationalist{% include sidenote.html id="mn-foundationalism" note='[Epistemological term](https://iep.utm.edu/foundationalism-in-epistemology/) referring to the idea of knowledge building on top of a foundation of other knowledge, gradually ascending as one gets to "stand on the shoulders of giants." This stance is implicitly baked into the structure of a logic proof (with premises being neatly separated, indicating some amount of epistemic privilege), but need not refer only to analytical expressions.'%} luxury of building on top of axiomatic premises, we risk the following failure mode. An opponent can simply contradict what the proponent says and win! It is quickly game over due to the opponent having the freedom not to build on the same foundation, rendering naive contrarianism into a winning strategy. Our second constraint on the search space is then the necessity to account precisely for this thorny problem. Who has the epistemic high ground when there is no absolute reference frame involved, when each party advocates their own? What can we substitute foundationalism with in order to gracefully handle such situations?

Following this second cut through the search space, we are fortunately left with more than nothing. Similar to how the geometrical conception of reasonableness incorporates the epistemological notion of foundationalism, most of the critical conceptions of reasonableness actually incorporate the epistemological notion of [coherentism](https://iep.utm.edu/coherentism-in-epistemology/). According to this view, it is those parties whose stances are coherent (e.g. which do not contradict themselves) which should get favored. Not only should the opponent undermine the proponent, but they should be making a good case for it, being able to stand resolutely against the inevitable counterattacks. In his [Introduction to Multiagent Systems](https://www.wiley.com/en-us/An+Introduction+to+MultiAgent+Systems%2C+2nd+Edition-p-9780470519462), Michael Wooldridge uses the phrasing _mutually defensive_ to describe a constellation of statements which effectively support each other in fending off attacks.{% include sidenote.html id="mn-wooldridge" note='A pioneer of multi-agent systems (recent spelling), Wooldridge has [collaborated with DeepMind](https://www.deepmind.com/publications/equilibrium-refinements-for-multi-agent-influence-diagrams-theory-and-practice) on topics not too far removed from the present work. When the individuals which make up a multi-agent system interact, the emergent phenomenon is intimately tied with dialectical formalisms we have previously touched on. We discuss this connection more in _Chapter 2_, when we further build on important work done at [DeepMind](https://www.deepmind.com/).'%} Laurence BonJour, a prominent epistemologist and proponent of coherentism, further expands this position to account for other ways of knowing. Belief systems which not only are internally coherent, but which are also coherent with perceived observations of the world—moving away from a potentially unhinged solipsism{% include sidenote.html id="mn-solipsism" note='Roughly, the philosophical position associated with living in one\'s head. Debate around [solipsism](https://en.wikipedia.org/wiki/Solipsism) in popular culture tends to focus on the implied loss of touch with reality, including the ignorance of one\'s alleged responsibility to contribute to the world.'%} while still steering clear of foundationalism—are even more promising. One could imagine further expanding this coherence heuristic{% include sidenote.html id="mn-heuristic" note='This somewhat technical term describes a "rule of thumb." Here, we are arguing about using a party\'s coherence as an indicator to provide the grounds for breaking the tie.'%} to the self-perception act involved in memory, another major way of knowing explored by epistemologists.

As the features of our algorithm become more prominent, we move from crudely eliminating large chunks of possibility space towards subtler polishing touches. What actually makes a position internally coherent? Conversely, what makes the opponent's position _not_ be coherent with the proponent's, as a prerequisite in undermining it? For a start, we might argue that statements which contradict each other are not coherent. In contrast, statements which generally support each other might be better described as such. This is not all, as a group of statements might also be argued to be coherent by virtue of coming together in the act of contradicting an external statement which threatens to undermine them all—the enemy of my enemy is my friend. It seems that those simpler notions of support and contradiction between statements, although composable in increasingly complex arrangements, can provide a basis for our notion of coherence. However, given the high bar we previously set for ourselves—to deal with the messiness of natural language, and through it, with that of the world at large—how could we estimate whether an arbitrary statement entails or contradicts another? Things get further complicated by the domain-specific knowledge required to evaluate many of those connections, as highlighted by Toulmin. The third constraint of our algorithm is therefore the ability to discern such relations between fragments of natural language as a basis for gauging coherence.

Fortunately, there already are systems out there which can help us determine how fragments of natural language relate to each other. Language models tasked with [natural language inference](https://paperswithcode.com/task/natural-language-inference)—the natural language processing task which involves determining whether a statement supports another, contradicts it, or none of the above—have recently achieved impressive performance.{% include sidenote.html id="mn-nlibench" note='The state-of-the-art on the Stanford Natural Language Inference (SNLI) benchmark was 93% in mid-2021, reports [Papers With Code](https://paperswithcode.com/sota/natural-language-inference-on-snli).'%} Those models have been optimized to match human-labelers in classifying hundreds of thousands of hand-crafted statement pairs, determining whether there is an (asymmetrical) entailment, contradiction, or neutral relation between them. The best models at the task tend to incorporate large amounts of unstructured knowledge gained through a previous "pretraining" stage, and are then "fine-tuned" to approximate human judgement in this more structured statement-statement-label setting.{% include sidenote.html id="mn-pretrain" note='We will cover the pretrain/fine-tune paradigm in more detail later on, as we train DebateGPT using a related approach in _Chapter 2_.'%} One might argue that the training process incentivizes the NLI models to _soak up domain-specific knowledge about which inferences are warranted_ as an instrumental goal in solving the task. Upon achieving high performance, the NLI models will have had, by necessity, internalized both (1) knowledge about the world, and (2) knowledge about whether its knowledge backs certain inferences and warrants certain conclusions. While NLI models will conveniently satisfy our present needs, they will prove limiting later on, as we set our sights on _superhuman_ reasoning. At the very end of _Chapter 2_, we will be forced to move beyond the "intelligence by proxy" trick involved in "merely" imitating human judgement, and explore more principled means of gauging coherence in order to get a grip, however loose, on the epistemic terra incognita.

However, the coherence of parties is more than the sum of individual relations between statements. What if each party contributed a dozen statements, some of which support each other and some of which are actively at odds with each other? Even worse, what if pairs which connect two different parties also vary wildy in their valence? What of the second-order effects briefly hinted at previously, with statements attacking a common enemy? What of higher-order effects? Who is to win when everybody is supporting each other to some extent, while also attacking everybody to a certain degree, while also reporting important amounts of in-fighting? We desperately need a way of making sense of this chaos.

Fortunately, network theorists have long grappled with problems involving countless elements being interweaved into the most complex of fabrics. Be it a billion people in a society, a billion pages on the internet, or a billion machines networked together digitally, network theory has helped us gain insight into the underlying structure of those systems. For instance, it can help determine whether people strongly rely on a certain factor when associating with others (e.g. assortative mixing by race), identify the most influential pages based on the support they garner from other influential pages (e.g. node centrality at early Google), or identify people similar to you based on whether they relate to other entities in a similar way (e.g. structural equivalence at early Facebook). Those applications are intimately tied to our present concerns, and it should come as no surprise that network theory has also been used in argumentation theory. As a prominent example, consider Phan Minh Dung's abstract argumentation systems, the ones Wooldridge was referring to when using the phrase _mutually defensive_. If one represents statements as nodes and the relations between them as directed edges, it then becomes possible to systematically identify relevant structures inside the argument graph. For instance, a set of arguments is said to be _admissible_ if and only if (1) it is conflict-free (i.e. there are no internal arguments which attack each other), and (2) the arguments are acceptable (i.e. for every argument which attacks it, there is one in the set which attacks it back).

<figure class="maincolumn">
    <figcaption class="marginnote">Diagram of an abstract argumentation system composed of seven statements. Five of them (i.e. green highlights) are part of a preferred, stable, and grounded extension, all technical terms denoting various properties of interest in the context of the argument graph. Figure lifted from the <a href="https://link.springer.com/referencework/10.1007/978-90-481-9473-5">Handbook of Argumentation Theory</a>.</figcaption>
    <img src="{{ site.baseurl }}/assets/img/aas.png"/>
</figure>

While Dung's formalism is extremely elegant and relevant to the issue of making sense of an interconnected fabric of arguments, it is not enough. The formalism has two important shortcomings. First, there is limited nuance in how one statement relates to a second (i.e it either attacks it or it does not). Second, the arguments themselves are similarly limited in terms of the "privilege" of being part of the quite exclusivist groupings (i.e. either an argument is part of the preferred extension or it is not). This general lack of nuance is detrimental in two ways. For one, it has trouble in handling the messiness of the world, with no possibility of a statement only lending _some_ degree of support to another one. In addition, it lacks _reward shaping_—the recognition of gradual, subtle, incremental shifts in reasonableness, a property essential for using it as part of a learning signal.

Fortunately, we can overcome both of these shortcomings relatively easily. Instead of using the NLI models as binary classifiers (i.e. "contradiction" versus "no contradiction", as Dung's formalism might suggest), and instead also of using them as ternary classifiers (i.e. for contradition, entailment, and neutral relations, as they are used in training), we take a step back from the discretized outputs and make use of the raw logits of the model.{% include sidenote.html id="mn-diff" note='Models need to be end-to-end differentiable in order to be optimized using gradient descent, so that they can take small steps towards being better at the task. This often means working with continuous functions, which is also the case for NLI models. Behind the label they output denoting the relation between the input pair of statements, there are three continuous numbers, one for each class. Turning them into a discrete label is trivial (i.e. just pick the one predicted to be most likely), but trying to hit a continuous target enables nuanced feedback, which in turn enables learning. This is also what we are trying to provide with the automated pipeline _for_ later models, but we are currently trying to extract this continuity from the previous models which _compose_ the pipeline, by bypassing the discrete classes and working with the "raw" class logits.'%} This allows us to _weigh_ the arcs which lead from one argument to another, using one number per arc, ranging from $$0.0$$ for a full-on attack to $$1.0$$ for full-on support, with $$0.5$$ denoting a generally neutral relation.{% include sidenote.html id="mn-calibration" note='There is a subtle issue when working with continuous outputs which have been trained in the context of discretized tasks. If the NLI model would output $$0.2$$ for "entailment," that does not mean that it estimates a $$20\%$$ chance that the input sentence pair captures an entailment. Those output values, also called pseudo-probabilities, are generally _not_ trained to be calibrated. Rather, they tend to be overconfident. There are many ways of calibrating output probabilities explored in the field of uncertainty quantification (e.g. condition outputs on empirical proportions: $$100$$ outputs of around $$0.2$$ for "entailment" should actually correspond to "entailment" being correct about $$20\%$$ of the time). However, they are rarely used in practice. This does not mean that a higher estimate for "entailment" does not correspond to the model gauging it as more likely, it is just that the value should not be interpreted as a probability, but merely as a rough signal.'%} Following this switch from directed edges to _weighted_ directed edges, we now attempt to replace Dung's black-or-white cliques with a fuzzier alternative to enable subtler evaluation of arguments, and by extension, of parties.

It turns out that simply applying Google's classic PageRank{% include sidenote.html id="mn-pagerank" note='Perhaps the most iconic algorithm for node centrality, the task of estimating the "authority" of each node in a graph. Originally developed for ranking web pages on early Google Search, [PageRank](https://en.wikipedia.org/wiki/PageRank) works by recursively nudging a page\'s rank based on the ranks of the pages which reference it. If many authoritative pages link to our page, then their "authority" will also "leak" into ours. But how can one know how authoritative those other pages were in the first place? Similarly, they might be referenced by other authoritative sources. This massive chicken-egg problem is solved by starting out with a baseline degree authority for each page, and conducting this "osmosis" until the node values convergence.'%} to the argument graph yields an evaluation which matches many of our previous intuitions. If one argument is overwhelmingly supported by many other arguments, then it receives a good rating. Not so much if those other arguments are systematically attacked, case in which it only gets a mediocre rating. Similarly, if one argument is overwhelmingly attacked by many other arguments, then it receives a low rating. Not so much if those other arguments are systematically attacked, case in which its rating is not hurt much. A group of arguments which support each other and systematically target external attackers find themselves in good standing. Ditto for strategically positioning oneself in order to derive support from the opponent. In contrast, a group of arguments which exhibit a lot of in-fighting relative to the support lent to third-parties will not find themselves in such a good standing. Ditto for stepping right into the opponent\'s line of fire. If we simply average the ratings held by all the utterances of a party, we finally obtain an estimate of the party's aggregate authority, similar to the authoritative sources promoted on search engines.

Notice also how capitalizing on the graph representation of the arguments contributed by parties fits with our shift away from foundationalism. In contrast to the quite linear structure of logic proofs, where each well-formed formula is built on the foundation of what came before it, starting off with the premises, the graph of arguments is inherently non-linear. There are no privileged or foundational nodes—there are just nodes. The notions of "above" and "below" are not well-defined across the flattened constellation of utterances. This has another added benefit. Namely, if at a later time we eliminate one particularly "dated" statement from a constellation, it will _not_ instantly bring down the entire structure built around it. The non-linear structure accounts for more than an epistemic [Jenga](https://en.wikipedia.org/wiki/Jenga), constantly on the brink of collapse. It can house self-sufficient and resilient belief systems, recursively supplying its own reason(s) for being. This "decentralized" flexibility allows us to deal with our final constraint—the accomodation of both beliefs as means and as ends, as mentioned in the previous section. While our algorithm is already well-equipped  deal with a brief encounter of parties (i.e. by providing the means of spotting the epistemic high-ground after a finite number of rounds), it can also allow for utterances to constantly pop in and out of a sliding window across time, enabling a constant scaffolding for the parties' transitions in belief. Besides, the homogeneity of the argument graph also levels the roles of the parties—one statement's proponent is another's opponent. There is no fundamental difference in motivation across parties, as each strives to attack the others while defending itself.

Let us retrace our steps. First, we wanted to be able to deal with reasonableness of arguments expressed in natural language. This led us to consider critical conceptions of reasonableness as a grounding for our algorithm. However, this naive approach raised the issue of contrarianism becoming an optimal strategy. To counter this, we resorted to coherentism as a stand-in for foundationalism. However, gauging coherence prompted us to consider feasible means of determining the way in which two fragments of natural language relate to each other. This tentatively led us to NLI models. However, the coherence of parties turned out to be more complex than the sum of how pairs of their statements relate. This prompted us to consider a network-theoretic approach as means of making sense of the chaos. Representing the interaction between parties as a graph also yielded the added benefit of enabling perpetuity, by having utterances pop in and out over time. Barring several tweaks which we will consider later in an attempt to access superhuman reasoning, we have largely completed our search for an algorithm. In the next section, we will put all of it together into a more concise form, leaving behind the motivating details of our interaction with possibility space.

### ArgRank

In this section, we summarize ArgRank, an algorithm for estimating reasonableness. ArgRank is based on a critical conception of reasonableness, one which favors those groupings of natural language arguments which systematically resist opponents which attempt to undermine them. Given this, we assume as a prerequisite the presence of several agents capable of deliberating in natural language about a range of topics (e.g. humans, human simulacra, etc.). Presently, we are not concerned with how we might engineer such agents—we turn to this task in _Chapter 2_. Instead, we are currently interested in a way of determining which party is "winning" in the first place, and by what margin. ArgRank attempts to provide a fuzzy estimate of each party's standing relative to the others, motivated by the epistemological and argumentation-theoretic considerations discussed earlier in the chapter.

ArgRank first represents the utterances of the parties-to-be-rated as nodes in an argument graph. To be more precise, the argument graph is a weighted, directed, and fully-connected graph. Each arc represents the relation between two utterances, with the arc's weight denoting the strength of the out-bound statement's support (or lack thereof) lent to the in-bound statement. The actual weight values are computed using a language model pre-trained to perform natural language inference (i.e. classify statement pairs as engaging in an entailment, contradiction, or neutral relation). We turn the three raw class logits returned by these models into one single arc weight by plugging the entailment and contradiction logits into a softmax,{% include sidenote.html id="mn-softmax" note='[Continuous function](https://en.wikipedia.org/wiki/Softmax_function) which takes in a list of real values and maps them across the $$[0, 1]$$ interval, while at the same time accentuating their "contrasts" in proportion to a "temperature" parameter. A low temperature will elevate the highest input values to $$1.0$$, while the others not raising much above $$0.0$$. A high temperature will place the input values more "spaced" across the output range. The term _temperature_ is not a fluke, with the function being motivated by statistical physicists, [Boltzmann especially](https://en.wikipedia.org/wiki/Boltzmann_distribution). The term _softmax_ highlights (1) the conceptual similarity to the simple `max` function, and (2) the fact that it is differentiable, in contrast to `max`.'%} and taking the first resulting value, similar to [another related application](https://discuss.huggingface.co/t/new-pipeline-for-zero-shot-text-classification/681). This has the effect of assigning values close to $$0.0$$ for a strong attack, and values close to $$1.0$$ for strong support being lent, with values close to $$0.5$$ denoting a more neutral relation.

Following the use of NLI models for weighing arcs, we then apply PageRank on the argument graph. This subroutine incorporated into ArgRank as-is assigns one numerical value to each utterance node. This can be interpreted as that statement's authority, with e.g. statements which are supported by other well-supported statements receiving a high rating. It is interesting to note that the sum of ratings is $$1.0$$, due to PageRank "preserving" the total amount of authority which is being iteratively passed around. Following this, we average the ratings of all the utterances contributed by each party, thus obtaining one single aggregate measure of reasonableness per party. Normalizing by the number of utterances per party, we obtain party ratings which also neatly sum to $$1.0$$. Finally, for a long deliberation, we can only include the last $$n$$ utterances contributed by each party as a "moving average" of the on-going situation.

This is the meat of ArgRank—essentially PageRank on the NLI-mediated argument graph, aggregated by party. The algorithm is quite straightforward in retrospect, yet required several conceptual leaps to reach, as the preceding sections attest. However, ArgRank first requires arguments to rate. Coming up with arguments effectively—using each utterance as a strategic move to further your party's standing—is an altogether different matter. It involves identifying your opponent's epistemic weak points, crafting strong arguments to target them, and fending off the imminent counterattacks. In _Chapter 2_, we turn towards creating an automated "strategist" to carry out such intricate manuevers, a process also known as debating. As we shall see, pitting it against against its _own_ past arguments, in an uneasy turn of events, will prove essential to the process.

## Ch. II, Deliberative Arms Race

### Brief Review of Language Models
