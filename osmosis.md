---
layout: post
title: Osmosis
description: Three sketches in identity.
---

For a while now, it has been fairly common to conceive of notions such as identity, personality, and self in terms of a narrative. One allegedly looks back on their personal history, the sequence of experiences they lived through, and perceives it as a storyline. This narrative is interweaved with those of their friends, family, and the other nested communities they find themselves in. However, at the end of the day, it is still one single coherent thread which you can follow from one moment to the next.

The coherence of such experiential trails doesn't appear out of nowhere. It is actively built, or at least actively perceived. People tend to behave in ways which are generally coherent with their story, or so the theory goes. Their self-concept, identified here with their narrative, serves as a set of implicit guidelines for how to act. This might or might not come out of a need to be seen as consistent by others. It might also be informed by an innate drive for coherence and harmony, manifesting itself in the top-down aspect of self-perception (i.e. the beholder's role).{% include sidenote.html id="mn-phonemes" note='One becomes acutely aware of the resources required for conscious deliberation exactly when acting naturally suddenly becomes unavailable. For instance, a reliable pattern we\'ve been exposed to since birth is that languages tend to make use of a tiny set of phonemes. It\'s quite likely that the same phonemes will follow in the context of a certain language, and unlikely that exotic ones will spontaneously show up. Whenever I switch between English and Romanian, the phonemes used in the former have a tendency to linger around in my speech for a few moments. Breaking out of the coherence of a pattern as strong as speech leaves one stumbling around for a few moments while making an effort to complete the transition.'%} It might also come from a need to simplify future decision-making, as acting naturally appears easier than painstaking conscious deliberation. Regardless of its teleological reason for being, perceiving self-perception as an act of following the storyline seems to help explain an important part of day-to-day behavior.

This narrative of people following narratives has undergone an important plot twist in social circles centered around the development and usage of autoregressive sequence models of the likes of GPT. To help you see the connection yourself, let me list some of their relevant characteristics. Such models are generally being fed an input prompt which consists of a sequence of discrete tokens. Those individual parts of the input sequence might perhaps represent words in a sentence, frames in a movie, or chords in a song. The model is trained to continue the sequence of tokens in coherent ways,{% include sidenote.html id="mn-metaphor" note='Like any metaphor, the anthropocentric one of models being trained to perform well on tasks highlights certain commonalities between the two while hiding others. For the purpose of the present piece, the metaphor turns out to be essential, as will soon become clear. However, it\'s still just one of many possible metaphors to be used in making sense of such models.' %} including by e.g. predicting tokens which were known to follow in an established sequence (e.g. words from a human-written book), by picking tokens which humans might find appropriate (e.g. [dialogue replies in a chat](https://openai.com/blog/chatgpt/)), or by various other techniques. Point is, after some amount training, the model is capable of continuing prompts in highly coherent ways, with "coherence" being defined implicitly and contextually, mostly by humans, partly by the world.

As another relevant observation, the training process appears to force the model to learn how to perceive the preceding sequence of prompts as an instrumental goal. It's not the case that the model blindly takes the whole prompt into consideration as an amorphous blob, as an agnosiac might fail to perceive objects despite still being able to pick up colors.{% include sidenote.html id="mn-agnosia" note='Perhaps the most prominent account of visual agnosia in popular culture is that of the man who mistook his wife for a hat, a neurological case which debuts Olver Sacks\'s eponymous anthology. The fascinating essay recounts the story of a music teacher who, upon leaving Sacks\' office, attempts to put his hat on by momentarily grabbing his wife\'s head. Dr. P struggles to correctly make out whole objects, yet is able to pick up their individual features. This does not prevent him from pursuing a successful career in music.'%} Rather, the model learns to distinguish relevant features of its prompt, perhaps in a similar way people focus on edges more than on solid surfaces.{% include sidenote.html id="mn-edges" note='The perception of edges in particular has been studied extensively in both humans and machines. Prior to the machine learning paradigm in AI, computer vision saw edges as "the holy grail" of object recognition. The idea was that if you could successfully pick up the outline of an object, its recognition would then be relatively straight-forward. This has definitely not been the case. In cognitive psychology, lateral inhibition is a celebrated finding about specific neural circuitry in early visual perception having been observed to render edges more salient. Hubel and Wiesel\'s early findings related to early neurons which respond differentially to edges with various orientations is another one.'%} Through a mix of top-down and bottom-up perception, it might start to discern the "materiality" of a song, the "texture" of a sentence, or the "tone" of a movie. Notably, this top-down component does not originate from the current prompt, although it's heavily informed by it, driven by it. It's what one pays (endogenous) attention to that interacts with what grabs their (exogenous) attention.

As a final relevant observation, if you swap a prompt for a different one, current such models tend to switch gears entirely and completely flip the script. Due to vast number of "threads" they've internalized during training, their inherent bias towards following certain dynamics more than others is relatively negligible. They're very willing to go with the flow, regardless of where the flow leads. This is likely to change slightly in the future, however, as researchers are exploring memory banks meant to provide representational resources halfway between the "hot" prompt and the "cold" weights.{% include sidenote.html id="mn-storage" note='I\'m making a parallel here to hot and cold storage in the context of digital infrastructure. An in-memory Redis cache has extremely fast read and write speeds, but constrained capacity, being deemed hot. A 5400 RPM HDD in a server farm on the other side of the world has cheaper storage capacity, but slow read and write speeds, being deemed cold.'%} Regardless, this cascade of memory systems can simply be seen as a broader specification of what dynamics to consider next, despite its temporal nature being less salient (e.g. non-episodic facts in a database).

To bring it together, the analogy goes as follows. The person is identified with the model. Their individual experiences are equated with the model's discrete tokens. Their personal history is associated with the model's prompt. The person's behavior is likened to the model's continuation of the prompt. A person's hypothetical rebirth followed by a different life is identified with swapping the model's prompt in various thought experiments. The formation of a self-concept by perceiving one's past is equated with the trained model perceiving its prompt. Most importantly, the person's narrative-driven behavior is associated with the model acting so as to achieve autoregressive coherence.{% include sidenote.html id="mn-projection" note='Psychology is perhaps the field where our projection of familiar ontologies onto the object of study is most obvious. Think of the "central executive" in twentieth-century American psychology, "neural circuits" in the past decades, or the upper hand of society\'s Nurture over the individual\'s Nature in soviet psychology. It is no surprise that this framing comes from people familiar exactly with such models.' %}

Leaving aside the dehumanizing aspect of identifying a person with a pile of matrices,{% include sidenote.html id="mn-derogatory" note='It\'s fascinating from an anthropological view to find historical precedent to snarky comments associating seemingly less agentic people with a stronger predisposition to narrative influences, similar to how an autoregressive model blindly follows its prompt. Born in 1763, Jean Paul authored a satirical essay titled <i>Humans Are the Machines of the Angels</i>, in which he alludes to the "intriguing" similarities between aristocratic mannerisms and the lifeless movements of clockwork mechanisms.' %} let's instead look at this colorful metaphor as merely a theoretical model meant to provide insight into human psychology. Just like any model, it's wrong. Not because it's utterly implausible, but simply because it's an incomplete picture of how the mind worksâ€“ there's always more to systems as messy and beautiful as people. However, like <i>some</i> theoretical models, the one we've been exploring might still prove useful by bringing to light several exciting affordances made available by the mechanics of mind. This is where we're going next, buying more into the theoretical model informally described above in an attempt to reap its practical fruit and gain new ways of constantly rebuilding ourselves.

## Attention Heads

A popular lower-level mechanism included in autoregressive sequence models is multi-headed attention. To put it briefly, the model employs multiple attention "heads" when perceiving its prompt, whose individual influences get added up to a final picture.{% include marginfigure.html id='mha' url='/assets/img/mha.png' description='The standard illustration of multi-headed attention tiles the multiple individual heads across the depth dimension, indexed by $$h$$, for heads.' %} One head might try to specifically pick up the topic of a sentence, the tempo of a song, or the protagonist in a movie. Each attention head only enables a select few well-chosen tokens to "leak" through into the model's behavior. By figuring out which features to ignore, the attention heads enables a few essential features to raise their voices above the noise.{% include sidenote.html id="mn-causal" note='One might argue, and by one I mean [Alexander Oldenziel and Adam Shai](https://www.lesswrong.com/posts/kqxEJkq5Big9nNKxy/beyond-kolmogorov-and-shannon), that the model is looking for a minimal description of the causal state it\'s in, and so only focuses on such definitive features. There are many superficial variations of the same theme which aren\'t essential in continuing a prompt.'%}

Interestingly enough, what each attention head looks for can be intervened on. You can theoretically go in, override the flow of information, and force the model to rely more on certain tokens than others. You can also add, remove, or swap attention heads on-the-fly relatively easily, though that's not common practice because we don't know how to hand-craft attention heads any better than the selective pressure of empirical risk minimization. Still, attention heads remain relatively compact building blocks of the model's self-perception mechanism.

To get us started in translating this mechanism into useful human practices, consider a series of classic studies on the effects of the availability heuristic on self-reported traits. Participants who found it easier to recall instances of their past selves manifesting a certain trait (e.g. assertiveness), tended to see themselves as possessing more of that trait when asked to self-rate. Various interventions such as priming, variable numbers of elicited experiences, delays, etc. have been adopted in experimental designs, and it seems that generally connecting oneself more strongly with curated past experiences can significantly shape our self-image in being more coherent with them.

Given that we could shape our self-image by simply steering those flows of information, and that our self-image in turn has an important impact on our behavior, what if we devised ways of better connecting with our best past selves in order to promote virtuous ways of being? Of course, actively building those experiences for our future selves to take inspiration from is a prerequisite, yet keeping them "close" might be a force multiplier.

One way of following through with this idea would be to first capture moments in which you manifested a virtue you want to cultivate, and then regularly browse through those instances. Those might be written notes persisted by your past self, pictures of the experienced surroundings, or "tag" scents playing the role of Proust's madelaine. Each such collection of a manifested virtue is associated here with an attention head making particular features of your past more prominent so that they can have a more decisive effect on day-to-day behavior without taxing your willpower.

<div class='epigraph'><blockquote><p>I place in position before my mind's eye the still recent taste of that first mouthful, and I feel something start within me, something that leaves its resting-place and attempts to rise, something that has been embedded like an anchor at a great depth; I do not know yet what it is, but I can feel it mounting slowly; I can measure the resistance, I can hear the echo of great spaces traversed.</p>
<footer>Marcel Proust, <cite>Swann's Way</cite></footer></blockquote></div>

Recently, I started implementing a related practice by collecting isolated instances of manifesting a certain virtue on a roll of analog film. The scarcity of available slots on a roll, commonly 24 or 36, makes it more intuitive to assign large "weights" to the few captured experiences, as opposed to a diluted quasi-infinite camera roll on a digital device.{% include marginfigure.html id='kodak' url='/assets/img/rolls.png' description='Standard Kodak case housing five film rolls.' %} Besides, having the entire "attention head" bundled up into a single compact capsule at the end of the process is also quite satisfying. After crafting more of such rolls, I'll probably tile them in a case, and pop one out for a quick trip back in time whenever I feel the need to "call on" certain past selves. The use of color reversal film also makes it possible to browse positive (i.e. non-negative) pictures without digitizing them, making for one of the few entirely non-digital projects I'm running.{% include sidenote.html id="mn-positives" note='As a technicality, of course positive images existed before the digital age. Think of all those dusty old pictures made last century. Apparently, film development back then involved taking another picture of the photographer\'s original picture, turning the original negative into a positive through what is essentially double negation. This practice is now virtually extinct, with labs simply inverting colors digitally after scanning, and so color reversal film seems the way to go if you want your pictures to both look normal and remain analog. Unfortunately, it tends to be a bit pricier and more picky when it comes to lighting conditions.'%}

Having the camera on hand also makes it feel like any "good deeds" I push myself to do have an additional benefit besides their inherent one. Not only would I be doing something I deem good, but I would also encourage my future self to follow along, knowing that they might very well be "listening in" through the information channels embedded in the rolls.{% include sidenote.html id="mn-dictum" note='Relatedly, I\'m currently experimenting with cultivating [this heuristic](https://sideways-view.com/2016/11/14/integrity-for-consequentialists/), which appears to be a secular relative of the more theologically-charged dictum of factoring in an all-seeing eye into one\'s day-to-day decision-making.'%} This additional nudge occasionally offsets the effort required by the one-off initial step of crafting a lositive example. One might also see this practice as a process of building identity capital, except seen through an engineering lens instead of an economics one.

## Residual Connections

Another mechanism often included in such models consists of residual connections, occasionally referred to as skip connections. Those are pathways which make early information directly available later in the model's processing pipeline by skipping over entire layers. They are special channels which allow older model states to directly have a say on the latest one, bypassing entire chunks of computation before feeding back at a junction. The boost in performance and stability they tend to bring about has been argued to be caused by the way they preserve access to "raw" structure throughout processing, structure which might only become useful in later stages of perception. In a sense, they're freeing the model from having to persist that raw early information through every single computation it undertakes, by simply setting up a direct parallel pathway.{% include marginfigure.html id='residuals' url='/assets/img/residuals.png' description='Residual connections (bottom), also called skip connections in older literature, pictured here bypassing two chunks of processing (top). Circles denote junctions where the residual connections feed back into the main processing stream. Diagram lifted from a [Distill article](https://distill.pub/2020/circuits/branch-specialization/#figure-2).' %}

Residual connections as employed in contemporary architectures tend to focus on propagating early model states across a single step of prompt continuation. Even in the original transformer diagram, those fast-tracked information highways are tightly sealed inside one stage of the process of taking in a sequence of tokens and picking a new one to tack on the ending. It might seem like the only way for the model to communicate across distinct token-generation stages is by emitting tokens which implicitly become the input of future model instances, working on taking the prompt for there.

However, this is really not the case, and the reason why is fascinating. There's an entire network of secret passages of information hiding in plain sight, and my feeling is that currently the majority of people working on such models are unaware of them. There's a surprising interaction between engineering and research, two pieces of the puzzle required to see through the illusion of prompt-as-sole-channel. In reality, models already have more persistent memory than we think.

To understand why, first consider the main architecture which currently powers autoregressive sequence models. Transformers work by mapping the (unordered) set of input tokens to a set of internal representations, then repeatedly map those to other internal representations, before finally mapping "out" to output tokens, all while preserving set cardinality. When running those models in inference mode and, say, producing token $$n+1$$ based on a history of $$n$$ tokens, the processing associated with propagating the $$n$$ old tokens through the model is not carried out again in full. Old intermediate representations get cached, for efficiency reasons. That's why, if you have a long prompt, it takes a while to get the generation started, but then it can stream tokens back much faster. It would be extremely expensive to actually recompute everything at every single step.{% include sidenote.html id="mn-histories" note='As a sidenote, you _can_ change those past internal representations if you really intend to, leading to some interesting (yet costly) applications. It\'s just fairly rare. For instance, [Dathathri et al.](https://arxiv.org/pdf/1912.02164.pdf) write:<br/><br/><i>"Taking steps in $$H_t$$ space leads to gradual changes to model activations â€” which may be thought of as gradual reinterpretations of the past â€” that guide future generation in the desired direction."</i><br/><br/>They are essentially proposing a "revisionist" history of past states which can subjected to optimization in order to retroactively steer the model\'s behavior. Using the ontology of the present essay, this can be seen as getting a handle on self-perception through the bottom-up route of changing the raw past states to be perceived, rather than through the beholder\'s top-down way in which past states are perceived.<br/><br/>This is perhaps reminiscent of experiments in cognitive psychology which resulted in manufactured memories by means of forged past photographs. One [particularly disturbing study](https://www.jstor.org/stable/24543655) managed to convince participants that they commited a crime which escalated to contact with law enforcement.'%}

However, while primarily meant as a reasonable engineering boon, this caching trick has a surprising effect on the abstract interpretation of what the model is doing. Because the past "thoughts" involved in the perception of past tokens is still available to it in the present moment, it can directly attend to its past thoughts from up to $$n$$ steps ago. Those steps don't have to be embedded in the prompt as an intermediate representation, they can simply be referenced in their native high-dimensional dialect. The distance between its thoughts across time gets smaller, requiring fewer intermediate hops to access. Make no mistake, if during training a similar caching mechanism is involved, the model wouldn't hesitate to learn to take advantage of this property and use it to perform better on its task, just like recent models reportedly exploited mixed precision of floats for squeezing out more computation. Those behaviors would be selected for under optimization pressure, just like any other high-fitness ones from the batched population of internal dynamics.

To recap, residual connections help fast-track older states, providing the latest model incarnation with easy access to raw past states. This might be across sequential processing of a single prompt (planned), or across various steps in continuing the prompt at different points (unplanned).

How might this kind of pattern translate to humans? Until relatively recently in humanity's history, we'd have almost no way of directly sending our future selves messages. We had to make do with using our memory or that of those around us. Perhaps it is also because of this relative novelty that we are so often intrigued by the possibility interacting with our past and future selves. We might address a written letter to ourselves-one-year-from-now, which we might then delight ourselves upon receiving, fascinated by the slightly unfamiliar modes of being manifested in it. We might do the same in the form of a video entry, enabling our future selves not only to rethink forgotten thoughts, but to watch our past selves think through them with us in real-time. Being able to put together plans spanning years is appealing, taking advantage of both your wiser future self and the initial ambition of a past one, scheming together with both the multiplicity and unity that is you.

A practice which fits the theme perfectly is that of "taking mnestics." Building on a trippy short story in which the protagonists are trying to fight monsters which tend to evade one's memory, Jarred Filmer proposes a [ritualized practice](https://theapolloalmanac.substack.com/p/mnestics) involving regularly sending and receiving messages across time. Unlike the story, the practice thankfully doesn't actually involve taking any paranormal meds, but instead frames the "mnestic" as a brief written meditation on forgetfulness, followed by a concise account of one's past ambitions and aspirations. While it might appear little more than a glorified journaling habitâ€”after all, Filmer's piece is almost as flowery as this oneâ€”the practice of regularly taking mnestics places emphasis on preserving a direct information pathway between past and future selves by acknowledging our natural tendency to lose track of what we deem important. It's meant to enable long-term coherence by placing our distant selves in synchrony.

The ritualized nature of the practice seems to fit the rhythmicity, the cadence of information hopping between regular junctions. There are several related parameters involved in designing the routine. What time periods ought to separate the junctions? Should they be weekly, monthly, quarterly, etc.? How many past messages should one take into account during the practice? Just the last one, or perhaps a notice board slowly put together during the last several occasions? Should pathways cross ways before feeding back in? Perhaps you'd send messages every week but only read from the second-to-last session two weeks ago. Perhaps one could use a statement about forgetfulness as the object of a short meditation before going through the incoming messages and sending new ones, instead of just rereading a document. Perhaps they could literally be phrased as brief text messages, or perhaps more like a stream of consciousness? Essentially hyperparameter tuning.

Of course, forgetfulness has its part, a point which many authors tried to convey, with Ted Chiang and Anthony Doerr as contemporary examples. How can one progress without leaving the past behind? There's probably a balance to be struck here, though I tend to see it as consisting of slightly less forgetfulness than usual, albeit not completely devoid of it.

<div class='epigraph'><blockquote><p>But in vain I set out to visit the city: forced to remain motionless and always the same, in order to be more easily remembered, Zora has languished, disintegrated, disappeared. The earth has forgotten her.</p>
<footer>Italo Calvino, <cite>Invisible Cities</cite></footer></blockquote></div>

## Cross Attention

Even if there's some truth to the narrative model of human identity, it's not as if our own thread of past experiences is the only one shaping our behavior.{% include sidenote.html id="mn-homonyms" note='Fun fact: <i>story</i> and <i>history</i> are homonyms in French, Italian, Portugese, and Spanish, which is... not that surprising, yet points at an underlying compatibility of the two readings. Or perhaps points simply at the mentioned languages being related, who knows?' %} Our threads are deeply interwoven with those of others around us, including people we directly interact with, but also people we simply hear about indirectly. It's not only our memories of our past selves informing our way of being, but also memories of how others have been. Given our innate social drives, those other threads might have an effect of comparable size, despite the contemporary fantasy of transcending interdependence and gaining complete control over one's destiny. We're embedded into so many narratives besides our own, and each one nudges us into its respective coherence.

<div class='epigraph'><blockquote><p>I am not sure that I exist, actually. I am all the writers that I have read, all the people that I have met, all the women that I have loved; all the cities I have visited, all my ancestors.</p>
<footer>Jorge Luis Borges, <cite>Collected Fictions</cite></footer></blockquote></div>

But we're getting ahead of ourselves. Cross-attention refers to a mechanism enabling models to not only attend to their previous states (i.e. self-attention), but to also attend to other sequences. This makes it so that those other sequences can have a say in the model's future states.{% include marginfigure.html id='transformer' url='/assets/img/transformer.webp' description='The standard illustration of the original transformer architecture employed in machine translation. The target sentence decoder (right) attends to the source sentence encoder (left) through cross-attention (the middle arrows which bridge them). Many recent models are decoder-only, and so lack cross-attention.' %} The "birthplace" of transformers was machine translation, where models were tasked with sketching out a sentence while constantly "keeping an eye on" the sentence to be translated. Some recent variations involve updating internal "mental states" by constantly attending to some external sequence of e.g. images through a different channel. Some others involve attending to past related sequences persisted in a database while putting together the current completion. In general, other sequences beyond the model's prompt can also feed into its thought process and reshape its behavior.

When it comes to people, I'm not sure about the weight we implicitly assign to those internal and external influences, relative to each other. Tilt the scale too much towards external influence, and the zeitgeist's modes of being simply get entrenched in one's own dynamics. Tilt the scale too much the other way and you reliabilly get solipsistic loss of touch with reality. An interesting related practice I often bring up on the topic comes from Neal Stephenson's Anathem, in which a constellation of scholarly communities implement an unusual ritual. During the regularly celebrated "Apert," each community establishes contact with the world at large for ten days. However, the communities vary in terms of the frequency of "opening up." The Unarians celebrate Apert every year, but e.g. the Millenarians only do it every thousand years. Naturally, the former are more in sync with the outside world, but it's one of the latter which drifts off to unlock the mysteries of navigating parallel dimensions, or something. The communities in the story dial the intake of external influence up or down by organizing Apert more or less frequently.

Similarly, one could try getting a grip on this internal-external scale by following a version of Apert which fits their needs. This might mean modulating the attention paid to other threads through the ritual's frequency and duration. Similar to the previous sketch, there are several parameters involved. Should Apert be observed weekly, monthly, quarterly? How long and intense should the Apert itself be? Just skimming through the top updates since the last session or engaging more deeply with a handful of them? How about packaging the entire information package into a single document to be read on an ebook in order to preserve focus? Perhaps a sprinkle of feature phone romanticization would also fit the picture. Those details would best be tuned by trial and error based on whether one wants to be more in the know or have more space to wander off.{% include sidenote.html id="mn-negative" note="It's easy to think of Apert as a regular ritual to be added to one's routine. However, this framing misses the point somewhat. Apert is all about the moments when the ritual per se is not happening. It's about creating that negative space which makes it possible to tilt the scales and direct your own thread more intentionally. The scheduled moment of contact is but a side product, despite arguably being the most visible feature of the practice. It's easy to miss the forest for the trees."%}

There's another point to explore related to multiple threads being interweaved, namely their interaction. Two external narratives can be conflicting, eliciting and promoting opposite behaviors, making for a destructive interference of sorts. Reading about more conservative characters of a bygone era might provide dynamics to "counterweight" and balance an exceptionally intense progressive ethos, folding external influence on itself along a certain dimension. This other way of tilting the scales naturally relies on one's choice of sources of inspiration, together with the intensity of the associated connections. There can also be constructive interference, with two deeply compatible narratives eliciting the same behavior. Peers, multimedia artifacts, and autoregressive sequence models which rely on them all seem to contain plenty of threads to take into account when working with one's loom.

Bringing everything together, the narrative model of human identity, together with its recent adaptation based on ideas around autoregressive sequence models, hints at several new tools to add in one's identity sculpting arsenal. While there is related empirical evidence out there, it tends to only support the above behavior change interventions indirectly. Still, they remain intriguing practices to experiment with and iterate on.
